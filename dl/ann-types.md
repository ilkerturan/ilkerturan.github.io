# Yapay Sinir AÄŸÄ± TÃ¼rleri

## ğŸ“š Ä°Ã§indekiler
1. [GiriÅŸ](#giriÅŸ)
2. [Feedforward Neural Networks (Ä°leri Beslemeli Sinir AÄŸlarÄ±)](#1-feedforward-neural-networks-ileri-beslemeli-sinir-aÄŸlarÄ±)
3. [Convolutional Neural Networks (EvriÅŸimsel Sinir AÄŸlarÄ±)](#2-convolutional-neural-networks-evriÅŸimsel-sinir-aÄŸlarÄ±)
4. [Recurrent Neural Networks (Yinelemeli Sinir AÄŸlarÄ±)](#3-recurrent-neural-networks-yinelemeli-sinir-aÄŸlarÄ±)
5. [Long Short-Term Memory Networks (LSTM)](#4-long-short-term-memory-networks-lstm)
6. [Generative Adversarial Networks (Ãœretken Ã‡ekiÅŸmeli AÄŸlar)](#5-generative-adversarial-networks-Ã¼retken-Ã§ekiÅŸmeli-aÄŸlar)
7. [Autoencoders (OtokodlayÄ±cÄ±lar)](#6-autoencoders-otokodlayÄ±cÄ±lar)
8. [Transformer AÄŸlarÄ±](#7-transformer-aÄŸlarÄ±)
9. [KarÅŸÄ±laÅŸtÄ±rma Tablosu](#karÅŸÄ±laÅŸtÄ±rma-tablosu)

---

## GiriÅŸ

Yapay sinir aÄŸlarÄ±, insan beynindeki nÃ¶ronlarÄ±n Ã§alÄ±ÅŸma prensibinden esinlenerek geliÅŸtirilmiÅŸ makine Ã¶ÄŸrenmesi modellerindir. Her bir yapay sinir aÄŸÄ± tÃ¼rÃ¼, farklÄ± problem tÃ¼rlerini Ã§Ã¶zmek iÃ§in Ã¶zelleÅŸtirilmiÅŸ mimari ve Ã¶ÄŸrenme mekanizmalarÄ±na sahiptir.

---

## 1. Feedforward Neural Networks (Ä°leri Beslemeli Sinir AÄŸlarÄ±)

### ğŸ” Nedir?
En temel yapay sinir aÄŸÄ± tÃ¼rÃ¼dÃ¼r. Bilgi, giriÅŸ katmanÄ±ndan Ã§Ä±kÄ±ÅŸ katmanÄ±na doÄŸru tek yÃ¶nlÃ¼ akar ve geriye dÃ¶nÃ¼ÅŸ yoktur. Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±lar (Multilayer Perceptrons - MLP) bu kategorinin en yaygÄ±n Ã¶rneÄŸidir.

### ğŸ“Š Mimari YapÄ±
```
GiriÅŸ KatmanÄ± â†’ Gizli Katman(lar) â†’ Ã‡Ä±kÄ±ÅŸ KatmanÄ±
     (xâ‚, xâ‚‚, xâ‚ƒ)  â†’  [â—â—â—â—]  â†’  [â—â—]  â†’  (yâ‚, yâ‚‚)
```

### ğŸ’¡ KullanÄ±m AlanlarÄ±
- SÄ±nÄ±flandÄ±rma problemleri (spam tespiti, hastalÄ±k teÅŸhisi)
- Regresyon problemleri (fiyat tahmini, risk deÄŸerlendirmesi)
- Basit Ã¶rÃ¼ntÃ¼ tanÄ±ma gÃ¶revleri
- Finansal tahminleme
- Kalite kontrol sistemleri

### âœ… AvantajlarÄ±
- Basit ve anlaÅŸÄ±lÄ±r yapÄ±
- HÄ±zlÄ± eÄŸitim sÃ¼resi
- Evrensel yaklaÅŸÄ±klÄ±k teoremi sayesinde herhangi bir fonksiyonu Ã¶ÄŸrenebilme
- Az veri ile Ã§alÄ±ÅŸabilme
- DÃ¼ÅŸÃ¼k hesaplama maliyeti

### âŒ DezavantajlarÄ±
- KarmaÅŸÄ±k yapÄ±sal verilerde (gÃ¶rÃ¼ntÃ¼, ses) yetersiz kalma
- Uzamsal iliÅŸkileri yakalayamama
- Zamansal baÄŸÄ±mlÄ±lÄ±klarÄ± modelleyememe
- Ã‡ok fazla parametre gerektirebilme
- AÅŸÄ±rÄ± Ã¶ÄŸrenmeye (overfitting) eÄŸilimli

### ğŸ“Œ Ek Bilgiler
- Ä°lk yapay sinir aÄŸÄ± modeli olan Perceptron (1958) bu kategoridedir
- Backpropagation algoritmasÄ± ile eÄŸitilir
- Aktivasyon fonksiyonlarÄ±: ReLU, Sigmoid, Tanh

---

## 2. Convolutional Neural Networks (EvriÅŸimsel Sinir AÄŸlarÄ±)

### ğŸ” Nedir?
GÃ¶rsel verileri iÅŸlemek iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ sinir aÄŸlarÄ±dÄ±r. EvriÅŸim (convolution) iÅŸlemi ile gÃ¶rÃ¼ntÃ¼deki yerel Ã¶zellikleri algÄ±lar ve hiyerarÅŸik Ã¶ÄŸrenme yapar.

### ğŸ“Š Mimari YapÄ±
```
Girdi â†’ KonvolÃ¼syon â†’ Aktivasyon â†’ Havuzlama â†’ DÃ¼zleÅŸtirme â†’ Tam BaÄŸlÄ± â†’ Ã‡Ä±ktÄ±
[28Ã—28] â†’  [Filter]  â†’   [ReLU]   â†’ [MaxPool] â†’  [Flatten]  â†’   [FC]    â†’ [10]
```

### ğŸ’¡ KullanÄ±m AlanlarÄ±
- GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma (nesne tanÄ±ma, yÃ¼z tanÄ±ma)
- Nesne algÄ±lama ve segmentasyon
- Medikal gÃ¶rÃ¼ntÃ¼ analizi (MR, rÃ¶ntgen, CT)
- Otonom araÃ§ sistemleri
- Video analizi
- Sanat ve stil transferi
- Optik karakter tanÄ±ma (OCR)

### âœ… AvantajlarÄ±
- Uzamsal iliÅŸkileri Ã§ok iyi yakalama
- Parametre paylaÅŸÄ±mÄ± ile dÃ¼ÅŸÃ¼k parametre sayÄ±sÄ±
- Translasyon deÄŸiÅŸmezliÄŸi (nesne gÃ¶rÃ¼ntÃ¼nÃ¼n neresinde olursa olsun tanÄ±ma)
- Otomatik Ã¶zellik Ã§Ä±karÄ±mÄ±
- HiyerarÅŸik Ã¶ÄŸrenme (basit â†’ karmaÅŸÄ±k Ã¶zellikler)

### âŒ DezavantajlarÄ±
- YÃ¼ksek hesaplama gÃ¼cÃ¼ gereksinimi (GPU zorunlu)
- BÃ¼yÃ¼k veri seti ihtiyacÄ±
- EÄŸitim sÃ¼resi uzun
- Rotasyon ve Ã¶lÃ§eklemeye karÅŸÄ± hassasiyet
- Hiperparametre ayarlamasÄ± karmaÅŸÄ±k

### ğŸ“Œ Ek Bilgiler
- PopÃ¼ler mimariler: LeNet, AlexNet, VGG, ResNet, Inception, EfficientNet
- Transfer Ã¶ÄŸrenme ile az veri ile baÅŸarÄ±lÄ± sonuÃ§lar
- KonvolÃ¼syon katmanlarÄ± yerel baÄŸlantÄ±lÄ±dÄ±r
- Pooling katmanlarÄ± boyut azaltma saÄŸlar

---

## 3. Recurrent Neural Networks (Yinelemeli Sinir AÄŸlarÄ±)

### ğŸ” Nedir?
Zamansal veya sÄ±ralÄ± veriyi iÅŸlemek iÃ§in tasarlanmÄ±ÅŸ aÄŸlardÄ±r. DÃ¶ngÃ¼sel baÄŸlantÄ±lar sayesinde Ã¶nceki bilgiyi hafÄ±zada tutar ve mevcut girdiyle birleÅŸtirir.

### ğŸ“Š Mimari YapÄ±
```
    â†» (geri besleme dÃ¶ngÃ¼sÃ¼)
    â†“
xâ‚ â†’ [RNN] â†’ hâ‚ â†’ yâ‚
xâ‚‚ â†’ [RNN] â†’ hâ‚‚ â†’ yâ‚‚
xâ‚ƒ â†’ [RNN] â†’ hâ‚ƒ â†’ yâ‚ƒ
    (gizli durum taÅŸÄ±nÄ±r)
```

### ğŸ’¡ KullanÄ±m AlanlarÄ±
- DoÄŸal dil iÅŸleme (metin Ã¼retimi, Ã§eviri)
- KonuÅŸma tanÄ±ma
- MÃ¼zik Ã¼retimi
- Zaman serisi tahmini (hisse senedi, hava durumu)
- Video analizi
- El yazÄ±sÄ± tanÄ±ma
- Duygu analizi

### âœ… AvantajlarÄ±
- DeÄŸiÅŸken uzunluktaki dizileri iÅŸleyebilme
- Zamansal baÄŸÄ±mlÄ±lÄ±klarÄ± yakalama
- Parametre paylaÅŸÄ±mÄ±
- SÄ±ralÄ± karar verme sÃ¼reÃ§lerinde etkili
- Gizli durum ile bellek mekanizmasÄ±

### âŒ DezavantajlarÄ±
- Vanishing/Exploding Gradient problemi
- Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenememe
- Paralel iÅŸleme zorluÄŸu (eÄŸitim yavaÅŸ)
- EÄŸitim kararsÄ±zlÄ±ÄŸÄ±
- Hesaplama maliyeti yÃ¼ksek

### ğŸ“Œ Ek Bilgiler
- Backpropagation Through Time (BPTT) ile eÄŸitilir
- Bidirectional RNN: Ä°leri ve geri yÃ¶nde bilgi akÄ±ÅŸÄ±
- Gradient clipping ile gradient patlamasÄ± Ã¶nlenir
- Modern uygulamalarda LSTM ve GRU tercih edilir

---

## 4. Long Short-Term Memory Networks (LSTM)

### ğŸ” Nedir?
RNN'lerin geliÅŸtirilmiÅŸ halidir. Ã–zel kapÄ± mekanizmalarÄ± (forget, input, output gates) ile uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilir ve vanishing gradient problemini Ã§Ã¶zer.

### ğŸ“Š Mimari YapÄ±
```
      HÃ¼cre Durumu (Cell State)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â†’
         â†‘      â†‘       â†‘
    [Unut] [GÃ¼ncelle] [Ã‡Ä±kÄ±ÅŸ]
      KapÄ±sÄ±  KapÄ±sÄ±   KapÄ±sÄ±
         â†‘      â†‘       â†‘
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
         GiriÅŸ ve Ã–nceki Durum
```

### ğŸ’¡ KullanÄ±m AlanlarÄ±
- Makine Ã§evirisi
- Metin Ã¼retimi ve tamamlama
- KonuÅŸma sentezi
- Video aÃ§Ä±klama oluÅŸturma
- MÃ¼zik kompozisyonu
- Anomali tespiti (zaman serilerinde)
- Protein yapÄ±sÄ± tahmini

### âœ… AvantajlarÄ±
- Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilme
- Vanishing gradient problemine Ã§Ã¶zÃ¼m
- SeÃ§ici hafÄ±za mekanizmasÄ±
- RNN'den daha kararlÄ± eÄŸitim
- KarmaÅŸÄ±k zamansal Ã¶rÃ¼ntÃ¼leri yakalama

### âŒ DezavantajlarÄ±
- YÃ¼ksek hesaplama maliyeti
- Ã‡ok fazla parametre (RNN'in 4 katÄ±)
- EÄŸitim sÃ¼resi uzun
- Bellek tÃ¼ketimi fazla
- Overfitting riski yÃ¼ksek

### ğŸ“Œ Ek Bilgiler
- 1997'de Hochreiter ve Schmidhuber tarafÄ±ndan geliÅŸtirildi
- GRU (Gated Recurrent Unit): LSTM'in basitleÅŸtirilmiÅŸ versiyonu
- Bidirectional LSTM: Her iki yÃ¶nde de bilgi iÅŸleme
- Attention mekanizmasÄ± ile birlikte kullanÄ±lÄ±r

---

## 5. Generative Adversarial Networks (Ãœretken Ã‡ekiÅŸmeli AÄŸlar)

### ğŸ” Nedir?
Ä°ki sinir aÄŸÄ±nÄ±n (Ãœretici ve AyÄ±rt Edici) birbirine karÅŸÄ± yarÄ±ÅŸtÄ±ÄŸÄ± bir modeldir. Ãœretici yeni veri Ã¼retir, AyÄ±rt Edici ise gerÃ§ek ve sahte veriyi ayÄ±rt etmeye Ã§alÄ±ÅŸÄ±r.

### ğŸ“Š Mimari YapÄ±
```
Rastgele GÃ¼rÃ¼ltÃ¼ â†’ [ÃœRETÄ°CÄ°] â†’ Sahte Veri
                                    â†“
GerÃ§ek Veri â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [AYIRT EDÄ°CÄ°] â†’ GerÃ§ek/Sahte?
                                    â†‘
                              KayÄ±p Geri Besleme
```

### ğŸ’¡ KullanÄ±m AlanlarÄ±
- GÃ¶rÃ¼ntÃ¼ Ã¼retimi (yÃ¼z, sanat eseri, manzara)
- GÃ¶rÃ¼ntÃ¼ iyileÅŸtirme ve sÃ¼per Ã§Ã¶zÃ¼nÃ¼rlÃ¼k
- Stil transferi
- Veri artÄ±rma (data augmentation)
- Deepfake teknolojisi
- Metin-gÃ¶rÃ¼ntÃ¼ dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- Ä°laÃ§ molekÃ¼lÃ¼ tasarÄ±mÄ±
- Video Ã¼retimi

### âœ… AvantajlarÄ±
- GerÃ§ekÃ§i veri Ã¼retme kapasitesi
- Ã–rtÃ¼k olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± Ã¶ÄŸrenme
- Keskin ve detaylÄ± Ã§Ä±ktÄ±lar
- Denetimsiz Ã¶ÄŸrenme yapabilme
- YaratÄ±cÄ± uygulamalar iÃ§in ideal

### âŒ DezavantajlarÄ±
- EÄŸitim kararsÄ±zlÄ±ÄŸÄ± (mode collapse)
- Dengeleme problemi (Ãœretici vs AyÄ±rt Edici)
- YakÄ±nsama garantisi yok
- Hiperparametre hassasiyeti Ã§ok yÃ¼ksek
- DeÄŸerlendirme metrikleri subjektif

### ğŸ“Œ Ek Bilgiler
- 2014'te Ian Goodfellow tarafÄ±ndan geliÅŸtirildi
- PopÃ¼ler varyantlar: DCGAN, StyleGAN, CycleGAN, Pix2Pix
- Wasserstein GAN (WGAN): EÄŸitim kararlÄ±lÄ±ÄŸÄ±nÄ± artÄ±rÄ±r
- Progressive GAN: YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ gÃ¶rÃ¼ntÃ¼ Ã¼retimi
- Etik sorunlar ve deepfake riskleri

---

## 6. Autoencoders (OtokodlayÄ±cÄ±lar)

### ğŸ” Nedir?
Veriyi sÄ±kÄ±ÅŸtÄ±rÄ±p (kodlayÄ±p) tekrar orijinal haline getirmeyi (Ã§Ã¶zmeyi) Ã¶ÄŸrenen denetimsiz Ã¶ÄŸrenme modelidir. Boyut azaltma ve Ã¶zellik Ã¶ÄŸrenme iÃ§in kullanÄ±lÄ±r.

### ğŸ“Š Mimari YapÄ±
```
GiriÅŸ â†’ [ENCODER] â†’ Gizli Temsil (Bottleneck) â†’ [DECODER] â†’ Ã‡Ä±kÄ±ÅŸ
[784]  â†’  [256]   â†’      [32]                  â†’   [256]   â†’ [784]
                    (sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ)
```

### ğŸ’¡ KullanÄ±m AlanlarÄ±
- Boyut azaltma ve Ã¶zellik Ã§Ä±karÄ±mÄ±
- Anomali tespiti
- GÃ¶rÃ¼ntÃ¼ gÃ¼rÃ¼ltÃ¼ giderme
- GÃ¶rÃ¼ntÃ¼ sÄ±kÄ±ÅŸtÄ±rma
- Veri tamamlama (eksik veri doldurma)
- Ã–neri sistemleri
- YÃ¼z tanÄ±ma (Ã¶zellik Ã¶ÄŸrenme)

### âœ… AvantajlarÄ±
- Denetimsiz Ã¶ÄŸrenme (etiket gerektirmez)
- Etkili boyut azaltma
- Ã–zellik Ã¶ÄŸrenme otomatik
- GÃ¼rÃ¼ltÃ¼ye karÅŸÄ± dayanÄ±klÄ± modeller
- Veri sÄ±kÄ±ÅŸtÄ±rma

### âŒ DezavantajlarÄ±
- Ãœretici modeller kadar gerÃ§ekÃ§i deÄŸil
- Overfitting riski
- Orijinal veri ile aynÄ± daÄŸÄ±lÄ±mda Ã§Ä±ktÄ±
- Yeni veri Ã¼retme kabiliyeti sÄ±nÄ±rlÄ±
- Hiperparametre seÃ§imi kritik

### ğŸ“Œ Ek Bilgiler
- Variational Autoencoder (VAE): OlasÄ±lÄ±ksal yaklaÅŸÄ±m, daha iyi Ã¼retim
- Denoising Autoencoder: GÃ¼rÃ¼ltÃ¼ giderme iÃ§in
- Sparse Autoencoder: Seyrek temsil Ã¶ÄŸrenme
- Convolutional Autoencoder: GÃ¶rÃ¼ntÃ¼ler iÃ§in
- PCA'ya alternatif, daha gÃ¼Ã§lÃ¼

---

## 7. Transformer AÄŸlarÄ±

### ğŸ” Nedir?
Self-attention mekanizmasÄ± kullanan, sÄ±ralÄ± veriyi paralel iÅŸleyebilen modern mimaridir. RNN ve LSTM'lerin yerini almÄ±ÅŸtÄ±r. "Attention is All You Need" makalesi ile tanÄ±tÄ±lmÄ±ÅŸtÄ±r.

### ğŸ“Š Mimari YapÄ±
```
GiriÅŸ â†’ [Positional Encoding] â†’ [Multi-Head Attention] â†’ [Feed Forward]
                                          â†“
                                  [Add & Normalize]
                                          â†“
                                  [Encoder/Decoder]
```

### ğŸ’¡ KullanÄ±m AlanlarÄ±
- DoÄŸal dil iÅŸleme (GPT, BERT, T5)
- Makine Ã§evirisi
- Metin Ã¶zetleme
- Soru-cevap sistemleri
- GÃ¶rÃ¼ntÃ¼ iÅŸleme (Vision Transformer - ViT)
- Protein yapÄ±sÄ± tahmini (AlphaFold)
- Kod Ã¼retimi (Codex, Copilot)
- KonuÅŸma tanÄ±ma

### âœ… AvantajlarÄ±
- Paralel iÅŸleme (Ã§ok hÄ±zlÄ± eÄŸitim)
- Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± mÃ¼kemmel yakalama
- Self-attention ile iliÅŸkileri modelleme
- Transfer Ã¶ÄŸrenme iÃ§in ideal
- Ã–lÃ§eklenebilir mimari
- Her pozisyona doÄŸrudan eriÅŸim

### âŒ DezavantajlarÄ±
- Ã‡ok yÃ¼ksek hesaplama maliyeti
- Muazzam veri ihtiyacÄ±
- Bellek tÃ¼ketimi Ã§ok fazla
- Karesel karmaÅŸÄ±klÄ±k (sequence uzunluÄŸuna gÃ¶re)
- KÃ¼Ã§Ã¼k veri setlerinde etkisiz

### ğŸ“Œ Ek Bilgiler
- 2017'de Google tarafÄ±ndan geliÅŸtirildi
- BERT: Encoder tabanlÄ±, anlama odaklÄ±
- GPT: Decoder tabanlÄ±, Ã¼retim odaklÄ±
- T5: Encoder-Decoder, evrensel model
- Vision Transformer (ViT): GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma
- Efficient Transformers: Linformer, Performer (bellek optimizasyonu)

---

## KarÅŸÄ±laÅŸtÄ±rma Tablosu

| AÄŸ TÃ¼rÃ¼ | En Ä°yi OlduÄŸu Alan | EÄŸitim HÄ±zÄ± | Veri Ä°htiyacÄ± | Hesaplama Maliyeti |
|----------|-------------------|-------------|---------------|-------------------|
| **Feedforward** | Tablo verisi | âš¡âš¡âš¡ HÄ±zlÄ± | ğŸ“Š Az | ğŸ’° DÃ¼ÅŸÃ¼k |
| **CNN** | GÃ¶rÃ¼ntÃ¼ iÅŸleme | âš¡âš¡ Orta | ğŸ“ŠğŸ“Š Orta-YÃ¼ksek | ğŸ’°ğŸ’° Orta |
| **RNN** | KÄ±sa diziler | âš¡ YavaÅŸ | ğŸ“ŠğŸ“Š Orta | ğŸ’°ğŸ’° Orta |
| **LSTM** | Uzun diziler | âš¡ YavaÅŸ | ğŸ“ŠğŸ“ŠğŸ“Š YÃ¼ksek | ğŸ’°ğŸ’°ğŸ’° YÃ¼ksek |
| **GAN** | Veri Ã¼retimi | âš¡ Ã‡ok YavaÅŸ | ğŸ“ŠğŸ“ŠğŸ“Š Ã‡ok YÃ¼ksek | ğŸ’°ğŸ’°ğŸ’° Ã‡ok YÃ¼ksek |
| **Autoencoder** | Boyut azaltma | âš¡âš¡ Orta | ğŸ“Š Az-Orta | ğŸ’°ğŸ’° Orta |
| **Transformer** | NLP & BÃ¼yÃ¼k veri | âš¡âš¡ Orta-HÄ±zlÄ± | ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š Ã‡ok YÃ¼ksek | ğŸ’°ğŸ’°ğŸ’°ğŸ’° Ã‡ok YÃ¼ksek |

---

## SeÃ§im Rehberi

### Hangi AÄŸÄ± Ne Zaman KullanmalÄ±?

**Tablo/SayÄ±sal Veri:**
- Feedforward Neural Network (MLP)

**GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme:**
- CNN (sÄ±nÄ±flandÄ±rma, algÄ±lama, segmentasyon)

**Metin/Dil:**
- Transformer (modern uygulamalar)
- LSTM (kaynak kÄ±sÄ±tlÄ± durumlar)

**Zaman Serisi:**
- LSTM (uzun baÄŸÄ±mlÄ±lÄ±klar)
- RNN (basit problemler)
- Transformer (bÃ¼yÃ¼k veri)

**Yeni Veri Ãœretimi:**
- GAN (gerÃ§ekÃ§i Ã¼retim)
- VAE (kontrollÃ¼ Ã¼retim)

**Boyut Azaltma:**
- Autoencoder

**Anomali Tespiti:**
- Autoencoder
- LSTM

---

## SonuÃ§

Yapay sinir aÄŸÄ± tÃ¼rlerinin her biri farklÄ± problem tÃ¼rleri iÃ§in optimize edilmiÅŸtir. DoÄŸru aÄŸ seÃ§imi, probleminizin doÄŸasÄ±na, veri tipinize ve kaynaklarÄ±nÄ±za baÄŸlÄ±dÄ±r. Modern uygulamalarda genellikle hibrit yaklaÅŸÄ±mlar (CNN + LSTM, Transformer + CNN gibi) kullanÄ±larak daha gÃ¼Ã§lÃ¼ modeller oluÅŸturulmaktadÄ±r.

**Gelecek Trendler:**
- Transformer mimarisinin diÄŸer alanlara yayÄ±lmasÄ±
- Daha verimli modeller (model compression, pruning)
- Few-shot ve Zero-shot learning
- Multimodal modeller (metin + gÃ¶rÃ¼ntÃ¼ + ses)
- Neuromorphic computing

---

## Teknik Terimler SÃ¶zlÃ¼ÄŸÃ¼

### A
**Activation Function (Aktivasyon Fonksiyonu):** NÃ¶ronun Ã§Ä±kÄ±ÅŸÄ±nÄ± belirleyen matematiksel fonksiyon. ReLU, Sigmoid, Tanh gibi.

**Attention Mechanism (Dikkat MekanizmasÄ±):** Modelin girdi dizisinin hangi kÄ±sÄ±mlarÄ±na odaklanacaÄŸÄ±nÄ± Ã¶ÄŸrenmesini saÄŸlayan mekanizma.

**Autoencoder:** Veriyi sÄ±kÄ±ÅŸtÄ±rÄ±p tekrar geniÅŸleten, denetimsiz Ã¶ÄŸrenme modeli.

### B
**Backpropagation:** Hata geriye yayÄ±lÄ±mÄ±. AÄŸÄ±rlÄ±klarÄ± gÃ¼ncellemek iÃ§in hatanÄ±n geriye doÄŸru yayÄ±lmasÄ±.

**Batch Normalization:** Katmanlar arasÄ± veriyi normalize ederek eÄŸitimi hÄ±zlandÄ±ran teknik.

**Batch Size:** Bir eÄŸitim adÄ±mÄ±nda kullanÄ±lan Ã¶rnek sayÄ±sÄ±.

**Bottleneck:** Autoencoder'da verinin en sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ hali, en dÃ¼ÅŸÃ¼k boyutlu katman.

### C
**Convolutional Layer (EvriÅŸim KatmanÄ±):** GÃ¶rÃ¼ntÃ¼de yerel Ã¶zellikleri tespit eden filtre katmanÄ±.

**Cross-Entropy Loss:** SÄ±nÄ±flandÄ±rma problemlerinde kullanÄ±lan kayÄ±p fonksiyonu.

### D
**Decoder (Ã‡Ã¶zÃ¼cÃ¼):** SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ veriyi orijinal formuna dÃ¶ndÃ¼ren aÄŸ bÃ¶lÃ¼mÃ¼.

**Discriminator (AyÄ±rt Edici):** GAN'da gerÃ§ek ve sahte veriyi ayÄ±rt etmeye Ã§alÄ±ÅŸan aÄŸ.

**Dropout:** Overfitting'i Ã¶nlemek iÃ§in rastgele nÃ¶ronlarÄ± devre dÄ±ÅŸÄ± bÄ±rakma tekniÄŸi.

### E
**Embedding:** Kategorik veya yÃ¼ksek boyutlu veriyi dÃ¼ÅŸÃ¼k boyutlu sÃ¼rekli vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rme.

**Encoder (KodlayÄ±cÄ±):** Veriyi dÃ¼ÅŸÃ¼k boyutlu temsile dÃ¶nÃ¼ÅŸtÃ¼ren aÄŸ bÃ¶lÃ¼mÃ¼.

**Epoch:** TÃ¼m eÄŸitim verisinin bir kez aÄŸdan geÃ§irilmesi.

**Exploding Gradient:** GradyanlarÄ±n eÄŸitim sÄ±rasÄ±nda aÅŸÄ±rÄ± bÃ¼yÃ¼mesi problemi.

### F
**Feature Map:** KonvolÃ¼syon katmanÄ±nÄ±n Ã§Ä±ktÄ±sÄ±, Ã¶zellik haritasÄ±.

**Feedforward:** Bilginin sadece ileri yÃ¶nde aktÄ±ÄŸÄ± aÄŸ yapÄ±sÄ±.

**Filter/Kernel:** CNN'de kullanÄ±lan, Ã¶zellikleri tespit eden kÃ¼Ã§Ã¼k matris.

**Fine-tuning:** Ã–nceden eÄŸitilmiÅŸ modelin yeni gÃ¶rev iÃ§in ince ayar yapÄ±lmasÄ±.

### G
**GAN (Generative Adversarial Network):** Ãœretici ve ayÄ±rt edici aÄŸÄ±n yarÄ±ÅŸtÄ±ÄŸÄ± model.

**Gate (KapÄ±):** LSTM'de bilgi akÄ±ÅŸÄ±nÄ± kontrol eden mekanizma (forget, input, output gates).

**Generator (Ãœretici):** GAN'da yeni veri Ã¼reten aÄŸ.

**Gradient Descent:** KayÄ±p fonksiyonunu minimize etmek iÃ§in kullanÄ±lan optimizasyon algoritmasÄ±.

**Gradient Clipping:** Gradient patlamasÄ±nÄ± Ã¶nlemek iÃ§in gradyanlarÄ± sÄ±nÄ±rlama.

### H
**Hidden Layer (Gizli Katman):** GiriÅŸ ve Ã§Ä±kÄ±ÅŸ arasÄ±ndaki ara iÅŸlem katmanlarÄ±.

**Hyperparameter (Hiperparametre):** Ã–ÄŸrenme hÄ±zÄ±, batch size gibi modelin eÄŸitim Ã¶ncesi ayarlanan parametreleri.

### I
**Input Layer (GiriÅŸ KatmanÄ±):** Verinin aÄŸa girdiÄŸi ilk katman.

### K
**Kernel:** Bkz. Filter.

### L
**Learning Rate (Ã–ÄŸrenme HÄ±zÄ±):** AÄŸÄ±rlÄ±klarÄ±n ne kadar hÄ±zlÄ± gÃ¼ncelleneceÄŸini belirleyen parametre.

**Loss Function (KayÄ±p Fonksiyonu):** Modelin hatasÄ±nÄ± Ã¶lÃ§en matematiksel fonksiyon.

**LSTM (Long Short-Term Memory):** Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilen RNN tÃ¼revi.

### M
**Max Pooling:** En bÃ¼yÃ¼k deÄŸeri seÃ§erek boyut azaltma iÅŸlemi.

**Mode Collapse:** GAN'da Ã¼reticinin Ã§eÅŸitlilik kaybetmesi problemi.

**Multi-Head Attention:** Transformer'da farklÄ± temsil alt uzaylarÄ±na dikkat eden mekanizma.

### N
**Neuron (NÃ¶ron):** Sinir aÄŸÄ±nÄ±n temel hesaplama birimi.

**Node:** Bkz. Neuron.

### O
**Output Layer (Ã‡Ä±kÄ±ÅŸ KatmanÄ±):** Modelin sonucunu Ã¼reten son katman.

**Overfitting (AÅŸÄ±rÄ± Ã–ÄŸrenme):** Modelin eÄŸitim verisini ezberlemesi, genelleme yapamamasÄ±.

**Optimizer:** Adam, SGD, RMSprop gibi aÄŸÄ±rlÄ±k gÃ¼ncelleme algoritmalarÄ±.

### P
**Padding:** Girdi boyutunu korumak iÃ§in kenarlardan sÄ±fÄ±r ekleme.

**Parameter (Parametre):** Modelin Ã¶ÄŸrendiÄŸi aÄŸÄ±rlÄ±k ve bias deÄŸerleri.

**Perceptron:** En basit yapay nÃ¶ron modeli.

**Pooling:** Boyut azaltma ve Ã¶nemli Ã¶zellikleri koruma iÅŸlemi.

**Positional Encoding:** Transformer'da sÄ±ra bilgisini ekleme mekanizmasÄ±.

### R
**ReLU (Rectified Linear Unit):** f(x) = max(0, x) aktivasyon fonksiyonu.

**Recurrent:** DÃ¶ngÃ¼sel baÄŸlantÄ±lar iÃ§eren aÄŸ yapÄ±sÄ±.

**Residual Connection:** KatmanlarÄ± atlayan kÄ±sa yol baÄŸlantÄ±larÄ± (ResNet'te).

**RNN (Recurrent Neural Network):** Zamansal veriyi iÅŸleyen dÃ¶ngÃ¼sel aÄŸ.

### S
**Self-Attention:** Dizinin her elemanÄ±nÄ±n diÄŸer elemanlarla iliÅŸkisini hesaplama.

**Sequence:** SÄ±ralÄ± veri dizisi (metin, zaman serisi).

**SGD (Stochastic Gradient Descent):** Rastgele gradyan iniÅŸi optimizasyon algoritmasÄ±.

**Sigmoid:** S-ÅŸeklinde 0-1 arasÄ± Ã§Ä±kÄ±ÅŸ veren aktivasyon fonksiyonu.

**Softmax:** Ã‡Ä±ktÄ±larÄ± olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na dÃ¶nÃ¼ÅŸtÃ¼ren fonksiyon.

**Stride:** Filtre veya pooling penceresinin kaydÄ±rma adÄ±m sayÄ±sÄ±.

### T
**Tanh:** -1 ile 1 arasÄ± Ã§Ä±kÄ±ÅŸ veren aktivasyon fonksiyonu.

**Transfer Learning:** Ã–nceden eÄŸitilmiÅŸ modeli yeni gÃ¶revde kullanma.

**Transformer:** Self-attention mekanizmasÄ± kullanan modern mimari.

### U
**Underfitting:** Modelin yetersiz Ã¶ÄŸrenmesi, dÃ¼ÅŸÃ¼k performans gÃ¶stermesi.

### V
**Vanishing Gradient:** GradyanlarÄ±n giderek kÃ¼Ã§Ã¼lerek kaybolmasÄ± problemi.

**Variational Autoencoder (VAE):** OlasÄ±lÄ±ksal kodlama yapan autoencoder tÃ¼rÃ¼.

### W
**Weight (AÄŸÄ±rlÄ±k):** NÃ¶ronlar arasÄ± baÄŸlantÄ± kuvveti, Ã¶ÄŸrenilen parametre.

**Weight Decay:** AÄŸÄ±rlÄ±klarÄ± kÃ¼Ã§Ã¼k tutarak overfitting'i Ã¶nleme tekniÄŸi.

### KÄ±saltmalar
- **AI:** Artificial Intelligence (Yapay Zeka)
- **ML:** Machine Learning (Makine Ã–ÄŸrenmesi)
- **DL:** Deep Learning (Derin Ã–ÄŸrenme)
- **ANN:** Artificial Neural Network (Yapay Sinir AÄŸÄ±)
- **CNN/ConvNet:** Convolutional Neural Network
- **RNN:** Recurrent Neural Network
- **LSTM:** Long Short-Term Memory
- **GRU:** Gated Recurrent Unit
- **GAN:** Generative Adversarial Network
- **VAE:** Variational Autoencoder
- **NLP:** Natural Language Processing (DoÄŸal Dil Ä°ÅŸleme)
- **CV:** Computer Vision (BilgisayarlÄ± GÃ¶rÃ¼)
- **GPU:** Graphics Processing Unit (Grafik Ä°ÅŸlemci)
- **TPU:** Tensor Processing Unit (Tensor Ä°ÅŸlemci)
- **API:** Application Programming Interface
